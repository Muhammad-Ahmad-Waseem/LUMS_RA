{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install required libs\n",
    "#!pip install tensorflow==1.13.2\n",
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow-gpu==1.13.2\n",
    "!pip install keras-applications==1.0.7\n",
    "!pip install keras==2.2.4\n",
    "!pip install image-classifiers==1.0.*\n",
    "!pip install efficientnet==1.0.*\n",
    "!pip install h5py==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### please update Albumentations to version>=0.3.0 for `Lambda` transform support\n",
    "!pip install -U albumentations>=0.3.0 --user \n",
    "!pip install -U segmentation-models --user\n",
    "!pip install -q -U segmentation-models-pytorch --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3353c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860af018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available() # True/False\n",
    "\n",
    "# Or only check for gpu's with cuda support\n",
    "tf.test.is_gpu_available(cuda_only=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from tqdm import tqdm as tqdm\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import multiprocessing as mlt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18652457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#////////////////////////UTILITY FUNCTIONS \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        #print(image.shape)\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "    \n",
    "# helper function for creating RGB of ith mask\n",
    "def MasktoRGB(mask,color):\n",
    "    r = np.expand_dims((mask*color[0]),axis=-1)\n",
    "    g = np.expand_dims((mask*color[1]),axis=-1)\n",
    "    b = np.expand_dims((mask*color[2]),axis=-1)\n",
    "    img = np.concatenate((r,g,b), axis=-1)\n",
    "    return img.astype(int)\n",
    "\n",
    "def round_clip_0_1(x, **kwargs):\n",
    "    return x.round().clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#////////////////////////DATASET FUNCTION FOR DATALOADER (torch.utils)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "class Dataset:\n",
    "    CLASSES = ['built-up','background']\n",
    "    colors = [\n",
    "        (164, 113, 88), # Built-up\n",
    "        (255, 255, 255) # background\n",
    "        ]\n",
    "    mode_choices = ['multiclass', 'multilabel']\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir,\n",
    "            contr_dir,\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            mode = 'multiclass'\n",
    "    ):\n",
    "        assert (mode in self.mode_choices), \"mode type not found. Choose one of these: {}\".format(self.mode_choices)\n",
    "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in np.sort(os.listdir(images_dir))]\n",
    "        self.mask_paths = [os.path.join(masks_dir, mask_id) for mask_id in np.sort(os.listdir(masks_dir))]\n",
    "        self.contr_paths = [os.path.join(contr_dir, contr_id) for contr_id in np.sort(os.listdir(contr_dir))]\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_paths[i])\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        contr = cv2.imread(self.contr_paths[i])\n",
    "        contr = cv2.cvtColor(contr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # convert RGB mask to index \n",
    "        one_hot_map = []\n",
    "        contr_map = np.all(np.equal(contr, self.colors[0]), axis=-1)\n",
    "        one_hot_map.append(contr_map)\n",
    "        mask_map = np.all(np.equal(mask, self.colors[0]), axis=-1)\n",
    "        one_hot_map.append(mask_map)\n",
    "        back_map = np.all(np.equal(mask, self.colors[1]), axis=-1)\n",
    "        one_hot_map.append(back_map)\n",
    "        \n",
    "        one_hot_map = np.stack(one_hot_map, axis=-1)\n",
    "        one_hot_map = one_hot_map.astype('float32')\n",
    "        \n",
    "        labels = np.argmax(one_hot_map, axis=-1)\n",
    "\n",
    "        if self.mode == self.mode_choices[1]:\n",
    "            # extract certain classes from mask (e.g. cars)\n",
    "            masks = [(labels == v) for v in range(3)]\n",
    "            mask = np.stack(masks, axis=-1).astype('float')\n",
    "        else:\n",
    "            mask = labels\n",
    "\n",
    "        #print(mask.shape)\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1)).astype('float32')\n",
    "        if self.mode == self.mode_choices[1]:\n",
    "            mask = np.transpose(mask, (2, 0, 1)).astype('int64')\n",
    "\n",
    "        return image,mask.astype('int64')#,self.image_ids[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#////////////////////////Augmentations\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "def get_training_augmentation():\n",
    "    train_transform = [albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        albumentations.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albumentations.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        albumentations.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        #albumentations.IAAPerspective(p=0.5),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.CLAHE(p=1),\n",
    "                albumentations.RandomBrightness(p=1),\n",
    "                albumentations.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.IAASharpen(p=1),\n",
    "                albumentations.Blur(blur_limit=3, p=1),\n",
    "                albumentations.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        albumentations.OneOf(\n",
    "            [\n",
    "                albumentations.RandomContrast(p=1),\n",
    "                albumentations.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        albumentations.Lambda(mask=round_clip_0_1)\n",
    "    ]\n",
    "    \n",
    "    return albumentations.Compose(train_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albumentations.PadIfNeeded(384, 480)\n",
    "    ]\n",
    "    return albumentations.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d61aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#////////////////////////Pre Processing\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function\n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        albumentations.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return albumentations.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc54107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////////// Setting the parameters \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "data_dir = \"D://LUMS_RA//Seg_data\" # Path to Dataset\n",
    "output_dir = \"D://LUMS_RA//Models//Segmentation//trained_model//DeepLabV3+\" # Path to save model\n",
    "n_classes = 3\n",
    "BACKBONE = \"efficientnetb3\"\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_VAL = 16\n",
    "LR = 0.00004\n",
    "EPOCHS = 40\n",
    "mode = 'multiclass'\n",
    "mode_choices = ['multiclass', 'multilabel'] #See documentation for more details.\n",
    "threshold = 0.5 #Threshold for computation of tp,fp,fn,tn\n",
    "\n",
    "assert (mode in mode_choices), \"Mode can be only from one of these: {}\".format(mode_choices)\n",
    "#Define Network/ Models and it's parameters. See documentation for more details.\n",
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=n_classes, \n",
    "    activation=ACTIVATION\n",
    "    )\n",
    "\n",
    "#Pre-processing function to pre-process whole data for encoder\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=LR),\n",
    "])\n",
    "\n",
    "'''\n",
    "Define Loss function. See smp.losses for more details:\n",
    "Types required are as follows:\n",
    "Ground Truth : torch.tensor of shape(N,1,H,W) for binary mode, (N,H,W) for multiclass mode and (N,C,H,W) \n",
    "               for multilabel mode.\n",
    "Prediction : (N,1,H,W) for binary mode, (N,C,H,W) for multiclass mode and multilabel mode.\n",
    "\n",
    "To see details of mode, see documentation of smp library.\n",
    "'''\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')\n",
    "loss = dice_loss\n",
    "\n",
    "#Define metrics to compute\n",
    "metrics = {\"IOU Score\": smp.metrics.iou_score,\n",
    "           \"F1 Score\": smp.metrics.f1_score,\n",
    "           \"F_beta score\": smp.metrics.fbeta_score,\n",
    "           \"Accuracy\": smp.metrics.accuracy,\n",
    "           \"Recall\": smp.metrics.recall}\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f295850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////////// Visualize the data \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "imgs_dir = os.path.join(data_dir, 'images')\n",
    "segm_dir = os.path.join(data_dir, 'segmentations')\n",
    "contr_dir = os.path.join(data_dir, 'contours')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = Dataset(\n",
    "    imgs_dir, \n",
    "    segm_dir,\n",
    "    contr_dir,\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "image, mask = dataset[1]\n",
    "print(mask.dtype)\n",
    "#print(image.shape)\n",
    "#print(mask.shape)\n",
    "plt.imshow(mask[0]+2*mask[1]+3*mask[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////////// Training \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "ct_train = False\n",
    "overwrite = False\n",
    "\n",
    "images_train_dir = os.path.join(data_dir, 'images')\n",
    "buildings_train_dir = os.path.join(data_dir, 'segmentations')\n",
    "boundaries_train_dir = os.path.join(data_dir, 'contours')\n",
    "\n",
    "images_test_dir = os.path.join(data_dir, 'test_images')\n",
    "buildings_test_dir = os.path.join(data_dir, 'test_segmentations')\n",
    "boundaries_test_dir = os.path.join(data_dir, 'test_contours')\n",
    "\n",
    "if ct_train:\n",
    "    assert os.path.exists(output_dir), \"Model not found at path: {}\".format(output_dir)\n",
    "    model = torch.load(os.path.join(output_dir,'best_model.h5'), map_location=DEVICE)\n",
    "    print('Loaded pre-trained DeepLabV3+ model!')\n",
    "else:\n",
    "    assert os.path.exists(output_dir) && overwrite, \"Path already exists, cannot over write. Please set the overwrite flag to high, if you want to overwrite\"\n",
    "    if not :\n",
    "        os.makedirs(output_dir)\n",
    "    else:\n",
    "        print(\"Warining Model path {} already exists, overwriting it\".format(output_dir))\n",
    "    \n",
    "\n",
    "# Dataset for train images\n",
    "train_dataset = Dataset(\n",
    "    images_train_dir,\n",
    "    buildings_train_dir,\n",
    "    boundaries_train_dir,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "# Dataset for validation images\n",
    "valid_dataset = Dataset(\n",
    "    images_test_dir,\n",
    "    buildings_test_dir,\n",
    "    boundaries_test_dir,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=8)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n",
    "\n",
    "best_iou_score = 0.0\n",
    "print(\"Total Epochs: {}\".format(EPOCHS))\n",
    "for i in range(0, EPOCHS):\n",
    "    # Perform training & validation\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    model.train()\n",
    "    train_logs = {}\n",
    "    loss_meter = np.array([])\n",
    "    metrics_meters = {name : np.array([]) for name, func in metrics.items()}\n",
    "    verbose = True\n",
    "\n",
    "    with open(os.path.join(output_dir, 'train_logs.txt'), \"w\") as f:\n",
    "        f.write(\"Epoch #: Logs\" + \"\\n\")\n",
    "\n",
    "    with open(os.path.join(output_dir, 'valid_logs.txt'), \"w\") as f:\n",
    "        f.write(\"Epoch #: Logs\" + \"\\n\")\n",
    "\n",
    "    with tqdm(train_loader, desc=\"Train\", file=sys.stdout, disable=not verbose) as iterator:\n",
    "        for x, y in iterator:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model.forward(x)\n",
    "            loss_Val = loss(y_pred, y)\n",
    "            loss_Val.backward()\n",
    "            optimizer.step()\n",
    "            # update loss logs\n",
    "            loss_value = loss_Val.cpu().detach().numpy()\n",
    "            loss_meter = np.append(loss_meter, loss_value)\n",
    "            loss_logs = {\"DICE_LOSS\": np.mean(loss_meter)}\n",
    "            train_logs.update(loss_logs)\n",
    "\n",
    "            if mode == mode_choices[1]:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(y_pred,dim=1), y, mode=mode,\n",
    "                                                       threshold=threshold)\n",
    "            else:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(y_pred, dim=1), y, mode=mode,\n",
    "                                                       num_classes=n_classes)\n",
    "\n",
    "            # update metrics logs\n",
    "            for name, metric_fn in metrics.items():\n",
    "                metric_value = metric_fn(tp, fp, fn, tn).cpu().detach().numpy()\n",
    "                metrics_meter = metrics_meters[name]\n",
    "                metrics_meters[name] = np.append(metrics_meter, metric_value)\n",
    "\n",
    "            metrics_logs = {name: np.mean(values) for name, values in metrics_meters.items()}\n",
    "            train_logs.update(metrics_logs)\n",
    "\n",
    "            if verbose:\n",
    "                str_logs = ['{} - {:.4}'.format(k, v) for k, v in train_logs.items()]\n",
    "                s = ', '.join(str_logs)\n",
    "                iterator.set_postfix_str(s)\n",
    "\n",
    "    torch.save(model, os.path.join(output_dir, 'best_model.h5'))\n",
    "\n",
    "    with open(os.path.join(output_dir, 'train_logs.txt'), 'a') as f:\n",
    "        f.write(\"{} : {}\".format(i, str(train_logs)) + \"\\n\")\n",
    "\n",
    "    model.eval()\n",
    "    valid_logs = {}\n",
    "    loss_meter = np.array([])\n",
    "    metrics_meters = {name : np.array([]) for name, func in metrics.items()}\n",
    "    verbose = True\n",
    "\n",
    "    with tqdm(valid_loader, desc=\"Valid\", file=sys.stdout, disable=not verbose) as iterator:\n",
    "        for x, y in iterator:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                y_pred = model.forward(x)\n",
    "                loss_Val = loss(y_pred, y)\n",
    "\n",
    "            # update loss logs\n",
    "            loss_value = loss_Val.cpu().detach().numpy()\n",
    "            loss_meter = np.append(loss_meter, loss_value)\n",
    "            loss_logs = {\"DICE_LOSS\": np.mean(loss_meter)}\n",
    "            valid_logs.update(loss_logs)\n",
    "\n",
    "            if mode == mode_choices[1]:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(y_pred, dim=1), y, mode=mode,\n",
    "                                                       threshold=threshold)\n",
    "            else:\n",
    "                tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(y_pred, dim=1), y, mode=mode,\n",
    "                                                       num_classes=n_classes)\n",
    "\n",
    "\n",
    "            # update metrics logs\n",
    "            for name,metric_fn in metrics.items():\n",
    "                metric_value = metric_fn(tp, fp, fn, tn).cpu().detach().numpy()\n",
    "                metrics_meter = metrics_meters[name]\n",
    "                metrics_meters[name] = np.append(metrics_meter, metric_value)\n",
    "\n",
    "            metrics_logs = {name: np.mean(vals) for name, vals in metrics_meters.items()}\n",
    "            valid_logs.update(metrics_logs)\n",
    "\n",
    "            if verbose:\n",
    "                str_logs = ['{} - {:.4}'.format(k, v) for k, v in valid_logs.items()]\n",
    "                s = ', '.join(str_logs)\n",
    "                iterator.set_postfix_str(s)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'valid_logs.txt'), 'a') as f:\n",
    "        f.write(\"{} : {}\".format(i, str(valid_logs)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bade35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////////// Visualize Results on an image from Test Set \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "images_dir = os.path.join(args.data_dir, 'test_images')\n",
    "masks_dir = os.path.join(args.data_dir, 'test_segmentations')\n",
    "contour_dir = os.path.join(args.data_dir, 'test_contours')\n",
    "\n",
    "# Set device: `cuda` or `cpu`   \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_dataset = Dataset(\n",
    "    images_dir,\n",
    "    masks_dir,\n",
    "    contour_dir,\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "# load best weights\n",
    "best_model = torch.load(os.path.join(args.output_dir,'best_model.h5'), map_location=DEVICE)\n",
    "\n",
    "image, gt_mask = test_dataset[0]\n",
    "#gt_mask = np.transpose(gt_mask,(1,2,0))\n",
    "\n",
    "x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "#image = image.squeeze()\n",
    "image = np.transpose(image,(1,2,0))\n",
    "\n",
    "pred_mask = best_model(x_tensor)\n",
    "pr_mask = pred_mask.squeeze()\n",
    "pr_mask = pr_mask.detach().squeeze().cpu().numpy()\n",
    "pr_mask = np.argmax(pr_mask, axis=0)\n",
    "\n",
    "#print(pr_mask.shape)\n",
    "#pr_mask = np.transpose(pr_mask,(1,2,0))\n",
    "'''\n",
    "print(pr_mask.shape)\n",
    "print(gt_mask.shape)\n",
    "print(image.shape)'''\n",
    "gt_img = 3*gt_mask[0]+2*gt_mask[1]+gt_mask[2]\n",
    "pr_img = 3*(pr_mask == 0) + 2*(pr_mask == 1) + (pr_mask == 2)\n",
    "print(pr_img.shape)\n",
    "\n",
    "#print(name)\n",
    "visualize(\n",
    "    image=denormalize(image.squeeze()),\n",
    "    gt_mask=gt_img.astype(np.uint8),\n",
    "    pr_mask=pr_img.astype(np.uint8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb488394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////////// Evaluate on Images \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "imgs_dir = os.path.join(data_dir, 'images') # Input the path to images\n",
    "pred_dir = os.path.join(data_dir, 'predictions') # The path where outputs are saved\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load best weights\n",
    "model = torch.load(os.path.join(output_dir,'best_model.h5'), map_location=DEVICE)\n",
    "dataset = Dataset(\n",
    "    imgs_dir, \n",
    "    imgs_dir, \n",
    "    classes=CLASSES, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "i = 0\n",
    "with tqdm(dataset, file=sys.stdout) as iterator:\n",
    "    for it in iterator:\n",
    "        image, _,f_name = dataset[i]\n",
    "        x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "        image = image.squeeze()\n",
    "        image = np.transpose(image,(1,2,0))\n",
    "        pred_mask = model(x_tensor)\n",
    "        pr_mask = pred_mask.round().squeeze()\n",
    "        pr_mask = pr_mask.detach().squeeze().cpu().numpy()\n",
    "\n",
    "        if mode == smp.losses.constants.BINARY_MODE:\n",
    "            pr_mask = np.expand_dims(pr_mask, 0)\n",
    "\n",
    "        #print(pr_mask.shape)\n",
    "        pr_mask = np.transpose(pr_mask,(1,2,0))\n",
    "        pr_img = np.zeros((pr_mask.shape[0],pr_mask.shape[1],3))\n",
    "        for j in range(pr_mask.shape[2]):\n",
    "            pr_img = pr_img+ MasktoRGB(pr_mask[...,j],Dataset.colors[j])\n",
    "\n",
    "        #f_name = \"{}.png\".format(i+1)\n",
    "        save_path = os.path.join(pred_dir,f_name)\n",
    "        plt.imsave(save_path,pr_img.astype(np.uint8))\n",
    "        i = i+1\n",
    "        #print(\"Output for Image: {} has been saved!\".format(f_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//////////////////////// Evaluate on Tiff Files \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "input_imgs = os.path.join(data_dir, 'tiff_images') # Input the path to tiff images\n",
    "\n",
    "target_size = (256, 256)\n",
    "padding_pixels = 32\n",
    "padding_value = 0\n",
    "\n",
    "if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "model = torch.load(os.path.join(output_dir, 'best_model.h5'), map_location=DEVICE)\n",
    "imgs = [file for file in glob.glob(input_imgs) if file.endswith('.tif')]\n",
    "\n",
    "assert len(imgs) > 0, \"The number of images equal to zero\"\n",
    "\n",
    "num_processes = 1\n",
    "print(\"Running on {} images using {} parallel processes\".format(len(imgs), num_processes))\n",
    "\n",
    "pos = len(input_imgs.split('*')[0]) #Related to naming of output file\n",
    "\n",
    "args = [[img, DEVICE, model, args.output_dir, pos] for img in imgs]\n",
    "\n",
    "if num_processes > 1:\n",
    "    p = mlt.Pool(num_processes)\n",
    "    (p.map(process, args))\n",
    "    p.close()\n",
    "else:\n",
    "    for arg in args:\n",
    "        process(arg)\n",
    "print(\"Completed\")\n",
    "def process(args):\n",
    "    img_path, device, model, output_dir, pos = args\n",
    "\n",
    "    file_name = os.path.split(img_path)[-1].split('.')[0]\n",
    "\n",
    "    sub_dirs = os.path.split(img_path[pos:])[0]\n",
    "    out_path = os.path.join(output_dir,sub_dirs)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    save_path = os.path.join(out_path, file_name + \"_preds.png\")\n",
    "    chk_path = save_path.split('.')[0]+'.tif'\n",
    "    if os.path.exists(save_path) or os.path.exists(chk_path):\n",
    "        print(\"Prediction for file {} already exists, skipping..!\".format(file_name))\n",
    "        return\n",
    "    img = np.array(Image.open(img_path))\n",
    "    img = cv2.copyMakeBorder(img, padding_pixels, padding_pixels, padding_pixels, padding_pixels,\n",
    "                             cv2.BORDER_CONSTANT, value=padding_value)\n",
    "    src_im_height = img.shape[0]\n",
    "    src_im_width = img.shape[1]\n",
    "\n",
    "    cols = (math.ceil(src_im_height/target_size[0]))\n",
    "    rows = (math.ceil(src_im_width/target_size[1]))\n",
    "\n",
    "    print(\"Total {} patches for the given image {}\".format(rows*cols, file_name))\n",
    "    combined_image = np.zeros((cols*target_size[0], rows*target_size[1]), dtype=np.uint8) * 255\n",
    "    #useful_portion = np.zeros((src_im_height-2*padding_pixels, src_im_width-2*padding_pixels), dtype=np.uint8)\n",
    "\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "    preprocessing = get_preprocessing(preprocessing_fn)\n",
    "\n",
    "    x1, y1, idx = 0, 0, 0\n",
    "    while y1 < src_im_height:\n",
    "        y2 = y1 + target_size[0] + 2 * padding_pixels\n",
    "        while x1 < src_im_width:\n",
    "            x2 = x1 + target_size[1] + 2 * padding_pixels\n",
    "            img_crop = img[y1: y2, x1: x2]\n",
    "            pad_bottom = y2 - src_im_height if y2 > src_im_height else 0\n",
    "            pad_right = x2 - src_im_width if x2 > src_im_width else 0\n",
    "\n",
    "            if pad_bottom > 0 or pad_right > 0:\n",
    "                img_crop = cv2.copyMakeBorder(img_crop, 0, pad_bottom, 0, pad_right,\n",
    "                                              cv2.BORDER_CONSTANT, value=padding_value)\n",
    "\n",
    "            sample = preprocessing(image=img_crop)\n",
    "            image = sample['image']\n",
    "            image = np.transpose(image, (2, 0, 1)).astype('float32')\n",
    "            x_tensor = torch.from_numpy(image).to(device).unsqueeze(0)\n",
    "            pred_mask = model(x_tensor)\n",
    "            pr_mask = pred_mask.squeeze()\n",
    "            pr_mask = pr_mask.detach().squeeze().cpu().numpy()\n",
    "            mask = pr_mask.round()\n",
    "            sizex, sizey = target_size\n",
    "            patch = mask[padding_pixels:sizex + padding_pixels, padding_pixels:sizey + padding_pixels]\n",
    "            combined_image[y1: y1+target_size[1], x1: x1+target_size[0]] = patch\n",
    "\n",
    "            x1 += target_size[0]\n",
    "            idx += 1\n",
    "            print(\"Patch {} done for file {}\".format(idx, file_name))\n",
    "            #break\n",
    "        x1 = 0\n",
    "        y1 += target_size[1]\n",
    "        #break\n",
    "\n",
    "    #useful_portion = combined_image[:src_im_height-2*padding_pixels, :src_im_width-2*padding_pixels]\n",
    "    plt.imsave(save_path, combined_image[:src_im_height-2*padding_pixels, :src_im_width-2*padding_pixels])\n",
    "\n",
    "    print_line = \"Processing completed for {}..!\".format(file_name)\n",
    "    print(print_line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
